{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yMrEDmseSBGx",
   "metadata": {
    "id": "yMrEDmseSBGx"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G2mgzzL043qy",
   "metadata": {
    "id": "G2mgzzL043qy"
   },
   "source": [
    "#### Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455ccde5-1bff-481c-b48b-886eeb968c6f",
   "metadata": {
    "id": "455ccde5-1bff-481c-b48b-886eeb968c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading pandas-2.3.0-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "Successfully installed numpy-2.2.6 pandas-2.3.0 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.5\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "Downloading contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.2-cp312-cp312-macosx_10_13_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m721.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.2 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 pyparsing-3.2.3\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-1.41.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from plotly) (24.2)\n",
      "Downloading plotly-6.1.2-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading narwhals-1.41.1-py3-none-any.whl (358 kB)\n",
      "Installing collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-1.41.1 plotly-6.1.2\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/miniconda3/lib/python3.12/site-packages (from scipy) (2.2.6)\n",
      "Downloading scipy-1.15.3-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.15.3\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2025.1.31)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/miniconda3/lib/python3.12/site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/miniconda3/lib/python3.12/site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/miniconda3/lib/python3.12/site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Downloading hdbscan-0.8.40-cp312-cp312-macosx_10_13_universal2.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/miniconda3/lib/python3.12/site-packages (from bertopic) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/miniconda3/lib/python3.12/site-packages (from bertopic) (2.3.0)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/miniconda3/lib/python3.12/site-packages (from bertopic) (6.1.2)\n",
      "Collecting scikit-learn>=1.0 (from bertopic)\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/miniconda3/lib/python3.12/site-packages (from bertopic) (4.67.1)\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/miniconda3/lib/python3.12/site-packages (from hdbscan>=0.8.29->bertopic) (1.15.3)\n",
      "Collecting joblib>=1.0 (from hdbscan>=0.8.29->bertopic)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/miniconda3/lib/python3.12/site-packages (from plotly>=4.7.0->bertopic) (1.41.1)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from plotly>=4.7.0->bertopic) (24.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.0->bertopic)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/lib/python3.12/site-packages (from sentence-transformers>=0.4.1->bertopic) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/miniconda3/lib/python3.12/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading numba-0.61.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.2->umap-learn>=0.5.0->bertopic)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.1.31)\n",
      "Downloading bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
      "Downloading hdbscan-0.8.40-cp312-cp312-macosx_10_13_universal2.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.0-cp312-cp312-macosx_12_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading numba-0.61.2-cp312-cp312-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading hf_xet-1.1.3-cp37-abi3-macosx_11_0_arm64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp312-cp312-macosx_11_0_arm64.whl (26.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.2/26.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, safetensors, regex, pyyaml, MarkupSafe, llvmlite, joblib, hf-xet, fsspec, filelock, scikit-learn, numba, jinja2, huggingface-hub, torch, tokenizers, pynndescent, hdbscan, umap-learn, transformers, sentence-transformers, bertopic\n",
      "Successfully installed MarkupSafe-3.0.2 bertopic-0.17.0 filelock-3.18.0 fsspec-2025.5.1 hdbscan-0.8.40 hf-xet-1.1.3 huggingface-hub-0.32.4 jinja2-3.1.6 joblib-1.5.1 llvmlite-0.44.0 mpmath-1.3.0 numba-0.61.2 pynndescent-0.5.13 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.7.0 sentence-transformers-4.1.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.7.1 transformers-4.52.4 umap-learn-0.5.7\n",
      "Collecting python-louvain\n",
      "  Downloading python-louvain-0.16.tar.gz (204 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from python-louvain) (3.5)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.12/site-packages (from python-louvain) (2.2.6)\n",
      "Building wheels for collected packages: python-louvain\n",
      "  Building wheel for python-louvain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-louvain: filename=python_louvain-0.16-py3-none-any.whl size=9430 sha256=97ee956c393954890f0518c072ec5f73c8e6024e7e92b4e781f28a5ef893209a\n",
      "  Stored in directory: /Users/anjali/Library/Caches/pip/wheels/40/f1/e3/485b698c520fa0baee1d07897abc7b8d6479b7d199ce96f4af\n",
      "Successfully built python-louvain\n",
      "Installing collected packages: python-louvain\n",
      "Successfully installed python-louvain-0.16\n",
      "/opt/miniconda3/bin/python: No module named spacy\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/miniconda3/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/miniconda3/lib/python3.12/site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/miniconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/miniconda3/lib/python3.12/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/miniconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.7-cp312-cp312-macosx_11_0_arm64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl (42 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-macosx_11_0_arm64.whl (126 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.7/634.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading thinc-8.3.6-cp312-cp312-macosx_11_0_arm64.whl (839 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.4/839.4 kB\u001b[0m \u001b[31m827.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.3.0-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m973.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl (174 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, click, catalogue, blis, srsly, smart-open, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 preshed-3.0.10 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install plotly\n",
    "!pip install scipy\n",
    "!pip install vaderSentiment\n",
    "!pip install seaborn\n",
    "!pip install bertopic\n",
    "!pip install python-louvain\n",
    "!python -m spacy download en_core_web_sm --user\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11076869-dca4-42bd-9074-01240f08a9cc",
   "metadata": {
    "id": "11076869-dca4-42bd-9074-01240f08a9cc"
   },
   "source": [
    "#### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa614b9",
   "metadata": {
    "id": "7fa614b9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcommunity\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import community.community_louvain as community_louvain\n",
    "import plotly.graph_objects as go\n",
    "from community import community_louvain\n",
    "import spacy\n",
    "import re\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from umap import UMAP\n",
    "\n",
    "# Load Spacy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa46de0-d80a-4ecd-9aac-0bee839a9de0",
   "metadata": {
    "id": "4fa46de0-d80a-4ecd-9aac-0bee839a9de0"
   },
   "source": [
    "#### Loading and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2433a47",
   "metadata": {
    "id": "d2433a47"
   },
   "outputs": [],
   "source": [
    "with open(\"InvestmentClub_submissions_refined.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open(\"InvestmentClub_comments_refined.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "df_sub = pd.DataFrame(data1)\n",
    "df_com= pd.DataFrame(data2)\n",
    "\n",
    "df_com[\"unique_id\"] = df_com[\"parent_id\"].str.replace(\"t3_\", \"\").str.replace(\"t1_\", \"\")\n",
    "df_com[\"created_utc\"] = pd.to_datetime(df_com[\"created_utc\"], unit=\"s\")\n",
    "df_sub[\"created_utc\"] = pd.to_datetime(df_sub[\"created_utc\"], unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1124e51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1124e51",
    "outputId": "3cd12bca-7e32-4a30-f558-846047109324",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Length of Submissions Dataset\",len(df_sub))\n",
    "print(\"Length of Comments Dataset\",len(df_com))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f8364-69cb-445b-a83f-2fd401e5b988",
   "metadata": {
    "id": "f98f8364-69cb-445b-a83f-2fd401e5b988"
   },
   "source": [
    "#### Custom Data Overview: Inspecting Reddit Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa729b-a7dd-4678-9ef5-3f03a918f695",
   "metadata": {
    "id": "4aaa729b-a7dd-4678-9ef5-3f03a918f695"
   },
   "outputs": [],
   "source": [
    "def custom_info(df):\n",
    "    print(f\"<class 'pandas.core.frame.DataFrame'>\")\n",
    "    print(f\"RangeIndex: {df.shape[0]} entries, 0 to {df.shape[0] - 1}\")\n",
    "    print(f\"Data columns (total {df.shape[1]} columns):\")\n",
    "    print(f\"{' #':<4} {'Column':<35} {'Null Count':<20} {'Dtype'}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, col in enumerate(df.columns):\n",
    "        non_null_count = df[col].isna().sum()\n",
    "        dtype = df[col].dtype\n",
    "        print(f\"{i:<4} {col:<35} {non_null_count:<20} {dtype}\")\n",
    "\n",
    "    print(f\"\\ndtypes: {', '.join([f'{t}({list(df.dtypes).count(t)})' for t in df.dtypes.unique()])}\")\n",
    "    print(f\"memory usage: {df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35476ffd-2c25-4000-8ac9-fbd1f9b05b4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35476ffd-2c25-4000-8ac9-fbd1f9b05b4e",
    "outputId": "7165f12b-b6d2-47cf-a5f6-f28d9bedc6e1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_info(df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca937c1-d572-4da7-84b3-a55d4a63db7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ca937c1-d572-4da7-84b3-a55d4a63db7d",
    "outputId": "e0b7e07e-db8f-40ab-e21a-0417639fca15"
   },
   "outputs": [],
   "source": [
    "df_com.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafbe12-e1eb-41e3-b56e-082afb929165",
   "metadata": {
    "id": "daafbe12-e1eb-41e3-b56e-082afb929165"
   },
   "source": [
    "#### Merging Posts and Comments for Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2834685-ee84-4ae3-baea-20abf06d3e97",
   "metadata": {
    "id": "e2834685-ee84-4ae3-baea-20abf06d3e97"
   },
   "outputs": [],
   "source": [
    "# Columns to retain from Posts Dataset\n",
    "posts_columns = [\n",
    "    'id',\n",
    "    'author',\n",
    "    'title',\n",
    "    'created_utc',\n",
    "    'score',\n",
    "    'num_comments',\n",
    "]\n",
    "\n",
    "# Columns to retain from Comments Dataset\n",
    "comments_columns = [\n",
    "    'id',\n",
    "    'author',\n",
    "    'unique_id',\n",
    "    'parent_id',\n",
    "    'body',\n",
    "    'created_utc',\n",
    "    'score',\n",
    "    'link_id',\n",
    "    'ups',\n",
    "    'downs'\n",
    "]\n",
    "\n",
    "df_sub = df_sub[posts_columns]\n",
    "df_com = df_com[comments_columns]\n",
    "\n",
    "new_df = df_com[df_com['unique_id'].isin(df_sub['id'])]\n",
    "merged_data = new_df.merge(df_sub, left_on='unique_id', right_on='id', how='inner', suffixes=('_comment', '_post'))\n",
    "\n",
    "# Combine title and body columns into a single text column\n",
    "merged_data['text'] = merged_data['title'] + ' ' + merged_data['body']\n",
    "\n",
    "# merged_data = merged_data.sample(n=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096fbed-82c9-40f2-a00a-8b726483131c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8096fbed-82c9-40f2-a00a-8b726483131c",
    "outputId": "f0afed97-a3e1-48b3-aefe-b91771b45387"
   },
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0067f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05a0067f",
    "outputId": "6ccdbbed-4247-4684-bf0e-1def7bc351aa"
   },
   "outputs": [],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2lhK7lJ_wiF9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lhK7lJ_wiF9",
    "outputId": "65714c2b-a2d6-40f3-db47-63841a14c05e"
   },
   "outputs": [],
   "source": [
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dD21m2qR1zgB",
   "metadata": {
    "id": "dD21m2qR1zgB"
   },
   "outputs": [],
   "source": [
    "# Get the top 5 most active users based on interactions\n",
    "top_users = merged_data[\"author_comment\"].value_counts().head(5)\n",
    "top_5_users = top_users.index.tolist()\n",
    "\n",
    "# Filter dataset for interactions involving the top 5 users\n",
    "filtered_df = merged_data[merged_data['author_comment'].isin(top_5_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XUzPVQXY19Ev",
   "metadata": {
    "id": "XUzPVQXY19Ev"
   },
   "outputs": [],
   "source": [
    "# Initialize directed graph\n",
    "G_top_5_users = nx.DiGraph()\n",
    "\n",
    "# Add edges from filtered dataset\n",
    "for _, row in filtered_df.iterrows():\n",
    "    parent = row['author_comment']\n",
    "    child = row['unique_id']\n",
    "    if pd.notna(parent) and pd.notna(child):\n",
    "        G_top_5_users.add_edge(parent, child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q2EOitlI26kL",
   "metadata": {
    "id": "q2EOitlI26kL"
   },
   "outputs": [],
   "source": [
    "# Calculate centrality metrics\n",
    "degree_centrality = nx.degree_centrality(G_top_5_users)\n",
    "betweenness_centrality = nx.betweenness_centrality(G_top_5_users)\n",
    "closeness_centrality = nx.closeness_centrality(G_top_5_users)\n",
    "\n",
    "def get_node_positions(graph):\n",
    "    return nx.spring_layout(graph, seed=42, k=0.3)\n",
    "\n",
    "# Get positions for top 5 users\n",
    "pos_top_5_users = get_node_positions(G_top_5_users)\n",
    "\n",
    "def extract_plot_data(graph, pos):\n",
    "    node_x, node_y, node_labels = [], [], []\n",
    "    for node, (x, y) in pos.items():\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_labels.append(node)\n",
    "\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in graph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    return node_x, node_y, node_labels, edge_x, edge_y\n",
    "\n",
    "# Extract node and edge positions\n",
    "node_x, node_y, node_labels, edge_x, edge_y = extract_plot_data(G_top_5_users, pos_top_5_users)\n",
    "\n",
    "# Scale node sizes dynamically\n",
    "node_sizes = [degree_centrality[node] * 300 for node in G_top_5_users.nodes()]\n",
    "\n",
    "# Normalize degree centrality values for coloring\n",
    "min_dc, max_dc = min(degree_centrality.values()), max(degree_centrality.values())\n",
    "\n",
    "def create_degree_centrality_plot(node_sizes):\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5, color=\"gray\"),\n",
    "        hoverinfo=\"none\",\n",
    "        mode=\"lines\"\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(\n",
    "            size=node_sizes,\n",
    "            color=[degree_centrality[node] for node in G_top_5_users.nodes()],\n",
    "            colorscale=\"YlOrRd\",\n",
    "            colorbar=dict(title=\"Degree Centrality\"),\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        text=node_labels,\n",
    "        textposition=\"top center\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=\"Degree Centrality of Top 5 Users\",\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        margin=dict(b=0, l=0, r=0, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def create_btwn_closns_centrality_plot(title, centrality_values, color_scale):\n",
    "    \"\"\"Creates a Plotly graph for betweenness or closeness centrality with fixed node sizes.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5, color=\"gray\"),\n",
    "        hoverinfo=\"none\",\n",
    "        mode=\"lines\"\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=[centrality_values[node] for node in G_top_5_users.nodes()],\n",
    "            colorscale=color_scale,\n",
    "            colorbar=dict(title=title),\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        text=node_labels,\n",
    "        textposition=\"top center\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        margin=dict(b=0, l=0, r=0, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d93a6-043e-463c-89d5-6adb2d81f487",
   "metadata": {
    "id": "048d93a6-043e-463c-89d5-6adb2d81f487"
   },
   "source": [
    "### Closeness Centrality for the Top 5 Most Active Users\n",
    "\n",
    "Closeness Centrality for these top 5 users means that they have the shortest average distance to all other users in the subreddit community. They are positioned in the network in such a way that they can interact with or \"reach\" other users quickly (i.e., fewer steps are needed to connect to other users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3W7CqC9Z3Unl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "3W7CqC9Z3Unl",
    "outputId": "dceb3125-aaea-44b0-b7e2-e453f53a96ce"
   },
   "outputs": [],
   "source": [
    "# Create plots for each centrality measure\n",
    "fig_closeness = create_btwn_closns_centrality_plot(\"Closeness Centrality of Top 5 Users\", closeness_centrality, \"Blues\")\n",
    "\n",
    "# Show plots\n",
    "fig_closeness.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8UXF-Pqjw6ma",
   "metadata": {
    "id": "8UXF-Pqjw6ma"
   },
   "source": [
    "### Betweeness Centrality Analysis of the Top 5 Users\n",
    "\n",
    "Betweenness Centrality for these top 5 users measures how often they act as bridges between other users in the subreddit community. These users facilitate interactions by connecting different parts of the network, meaning they play a crucial role in information flow and community cohesion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csNz9hFM3g7C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "csNz9hFM3g7C",
    "outputId": "23079a0c-e877-4ed7-c49f-746dcb5f61f7"
   },
   "outputs": [],
   "source": [
    "# Create plots for each centrality measure\n",
    "fig_betweenness = create_btwn_closns_centrality_plot(\"Betweenness Centrality of Top 5 Users\", betweenness_centrality, \"Blues\")\n",
    "\n",
    "# Show plots\n",
    "fig_betweenness.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bfe61-0793-426a-ae11-a6f5c33408c3",
   "metadata": {
    "id": "927bfe61-0793-426a-ae11-a6f5c33408c3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Degree Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657FpEJ8xV5w",
   "metadata": {
    "id": "657FpEJ8xV5w"
   },
   "source": [
    "Degree Centrality for these top 5 users reflects the number of direct connections they have within the subreddit community. Users with higher degree centrality appear larger in the visualization, indicating their greater number of interactions. Additionally, their colors range from yellow to red, where deeper red hues correspond to higher degree centrality values. This visually highlights the most influential users in terms of direct engagement with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JWW-GLXa3o2I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "JWW-GLXa3o2I",
    "outputId": "93edc670-4b37-460f-9545-5946be31bba7"
   },
   "outputs": [],
   "source": [
    "# Create plots for each centrality measure\n",
    "fig_degree = create_degree_centrality_plot(node_sizes)\n",
    "\n",
    "# Show plots\n",
    "fig_degree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ceb9d6-0ceb-4898-b730-6c63c14d134f",
   "metadata": {
    "id": "15ceb9d6-0ceb-4898-b730-6c63c14d134f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Directed Interaction Graphs for the Top 5 Most Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3f0d-583e-4cc6-a7dc-52b3905f675b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c17d3f0d-583e-4cc6-a7dc-52b3905f675b",
    "outputId": "bed5039d-e1fb-43c5-fd77-e702a53f793e"
   },
   "outputs": [],
   "source": [
    "for user in top_5_users:\n",
    "\n",
    "    # Filter dataset for interactions involving this user\n",
    "    user_df = merged_data[(merged_data[\"author_comment\"] == user) | (merged_data[\"unique_id\"] == user)]\n",
    "\n",
    "    # Create a directed graph\n",
    "    G_user = nx.DiGraph()\n",
    "\n",
    "    # Add edges from the filtered dataset\n",
    "    for _, row in user_df.iterrows():\n",
    "        parent = row[\"author_comment\"]\n",
    "        child = row[\"unique_id\"]\n",
    "        if pd.notna(parent) and pd.notna(child):\n",
    "            G_user.add_edge(parent, child)\n",
    "\n",
    "    # Use a spring layout for better visualization\n",
    "    pos_user = get_node_positions(G_user)\n",
    "\n",
    "\n",
    "    # Extract node positions\n",
    "    node_x, node_y, node_labels = [], [], []\n",
    "    for node, (x, y) in pos_user.items():\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_labels.append(node)\n",
    "\n",
    "    # Extract edge positions\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in G_user.edges():\n",
    "        x0, y0 = pos_user[edge[0]]\n",
    "        x1, y1 = pos_user[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    # Create a directed graph visualization using Plotly\n",
    "    fig_user = go.Figure()\n",
    "\n",
    "    # Add edges (lines)\n",
    "    fig_user.add_trace(go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5, color=\"gray\"),\n",
    "        hoverinfo=\"none\",\n",
    "        mode=\"lines\"\n",
    "    ))\n",
    "\n",
    "    # Add nodes with labels\n",
    "    fig_user.add_trace(go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(size=12, color=[\"red\" if node == user else \"blue\" for node in node_labels], opacity=0.8),\n",
    "        text=node_labels,\n",
    "        textposition=\"top center\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "\n",
    "\n",
    "    fig_user.update_layout(\n",
    "        title=f\"Directed Interaction Graph for {user}\",\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        margin=dict(b=0, l=0, r=0, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    )\n",
    "\n",
    "\n",
    "    fig_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LlyWC6emKk_l",
   "metadata": {
    "id": "LlyWC6emKk_l"
   },
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex20i0IHYfLQ",
   "metadata": {
    "id": "ex20i0IHYfLQ"
   },
   "outputs": [],
   "source": [
    "# Custom stopword list\n",
    "with open(\"stopwordFile.txt\", \"r\") as f:\n",
    "    custom_stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Preprocess Text (remove stopwords, lemmatize, clean)\n",
    "def preprocess(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in custom_stopwords\n",
    "              and not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "merged_data['cleaned_text'] = merged_data['text'].fillna('')\n",
    "merged_data[\"cleaned_text\"] = merged_data[\"cleaned_text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2YXVhw-LYTSz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "2YXVhw-LYTSz",
    "outputId": "e31d8cc3-8e46-423c-aa8c-df02137cd634"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=list(custom_stopwords), ngram_range=(1, 2))\n",
    "X_tfidf = vectorizer.fit_transform(merged_data[\"cleaned_text\"])\n",
    "\n",
    "# UMAP for dimensionality reduction\n",
    "umap_model = UMAP(n_neighbors=20, n_components=10, min_dist=0.0, metric=\"cosine\")\n",
    "X_umap = umap_model.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# LDA Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(X_tfidf)\n",
    "\n",
    "# Extracting top words for each topic\n",
    "words = vectorizer.get_feature_names_out()\n",
    "topics_info = []\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    top_words_indices = topic.argsort()[-10:][::-1]\n",
    "    top_words = [words[j] for j in top_words_indices]\n",
    "\n",
    "    topic_label = f\"Topic {i+1}\"\n",
    "\n",
    "    topic_name = \", \".join(top_words)\n",
    "\n",
    "    # Create the count (frequency of each word in the topic)\n",
    "    word_count = [topic[j] for j in top_words_indices]\n",
    "\n",
    "    # Append the topic information to the list\n",
    "    topics_info.append([topic_label, topic_name, word_count])\n",
    "\n",
    "# Convert topics info into DataFrame\n",
    "topics_df = pd.DataFrame(topics_info, columns=[\"topic_label\", \"topic_name\", \"count\"])\n",
    "\n",
    "# Assign topics to the merged_data DataFrame\n",
    "topic_assignments = lda.transform(X_tfidf)\n",
    "merged_data[\"topic\"] = topic_assignments.argmax(axis=1)\n",
    "\n",
    "# Count the number of rows assigned to each topic\n",
    "topic_counts = merged_data['topic'].value_counts().reset_index()\n",
    "topic_counts.columns = ['topic_label', 'row_count']\n",
    "\n",
    "# Convert topic_label to string to match with topics_df\n",
    "topic_counts['topic_label'] = topic_counts['topic_label'].apply(lambda x: f\"Topic {x + 1}\")\n",
    "\n",
    "topics_df = topics_df.merge(topic_counts, on='topic_label', how='left')\n",
    "merged_data['topic'] = merged_data['topic'].astype(str)\n",
    "merged_data = merged_data.merge(topics_df[['topic_label', 'topic_name']], left_on='topic', right_on='topic_label', how='left')\n",
    "\n",
    "sorted_topics_df = topics_df[[\"topic_label\", \"topic_name\", \"row_count\"]].sort_values(by=\"row_count\", ascending=False)\n",
    "sorted_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunViqbMSE6",
   "metadata": {
    "id": "stunViqbMSE6"
   },
   "source": [
    "### Top Topics by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-IVE1ef-E4to",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "-IVE1ef-E4to",
    "outputId": "79901a92-d544-4e36-bcf4-b3bd76efb167"
   },
   "outputs": [],
   "source": [
    "# Create interactive bar chart\n",
    "fig = px.bar(\n",
    "    sorted_topics_df,\n",
    "    x=\"topic_label\",\n",
    "    y=\"row_count\",\n",
    "    text=\"row_count\",\n",
    "    hover_data=[\"topic_name\"],\n",
    "    labels={\"topic_label\": \"Topic\", \"row_count\": \"Number of Mentions\"},\n",
    "    title=\"Topic Mentions in Dataset\"\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition=\"outside\")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46vCpzprCbuc",
   "metadata": {
    "id": "46vCpzprCbuc"
   },
   "source": [
    "## Community Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rv0sq0KmZLIO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rv0sq0KmZLIO",
    "outputId": "87bf8618-5e85-4ee2-8046-3cd0cf3e3e67"
   },
   "outputs": [],
   "source": [
    "# Function to build graph for each topic\n",
    "def build_graph(df, topic):\n",
    "    G = nx.Graph()\n",
    "    topic_data = df[df[\"topic\"] == topic]\n",
    "\n",
    "    for _, row in topic_data.iterrows():\n",
    "        user = row[\"author_comment\"]\n",
    "        reply_to = row[\"unique_id\"]\n",
    "\n",
    "        G.add_node(user)\n",
    "\n",
    "        if pd.notna(reply_to):\n",
    "            G.add_node(reply_to)\n",
    "            G.add_edge(user, reply_to)\n",
    "\n",
    "    return G\n",
    "\n",
    "def plot_community_graph(G, topic):\n",
    "    if G.number_of_nodes() == 0:\n",
    "        print(f\"Skipping {topic} - No interactions\")\n",
    "        return\n",
    "\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    node_colors = [partition[node] for node in G.nodes()]\n",
    "\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=edge_x, y=edge_y, mode=\"lines\", line=dict(width=0.5, color=\"gray\"), hoverinfo=\"none\"))\n",
    "    node_x, node_y = zip(*[pos[node] for node in G.nodes()])\n",
    "    node_text = list(G.nodes())\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=node_x, y=node_y, mode=\"markers\",\n",
    "        marker=dict(size=10, color=node_colors, colorscale=\"Viridis\", showscale=True),\n",
    "        text=node_text, hoverinfo=\"text\"\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Community Graph for Topic: {topic}\",\n",
    "        showlegend=False, hovermode=\"closest\",\n",
    "        margin=dict(b=0, l=0, r=0, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Generate Graphs for Each Topic\n",
    "unique_topics = merged_data[\"topic\"].dropna().unique()\n",
    "for topic in unique_topics:\n",
    "    G = build_graph(merged_data, topic)\n",
    "    plot_community_graph(G, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ENk13WtwuiBB",
   "metadata": {
    "id": "ENk13WtwuiBB"
   },
   "outputs": [],
   "source": [
    "# topics_df.to_csv(\"topics_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TcbydNT6_oRx",
   "metadata": {
    "id": "TcbydNT6_oRx"
   },
   "source": [
    "### Community Talking about Warren Buffet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_vLzmghM_k-E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "_vLzmghM_k-E",
    "outputId": "7a10d137-6925-4e06-f85b-2d8d424e26ee"
   },
   "outputs": [],
   "source": [
    "# Filter relevant comments\n",
    "rel_comments = merged_data[\"cleaned_text\"].str.contains(r\"\\b(warren|buffet|warren buffet)\\b\", case=False, na=False)\n",
    "\n",
    "# Count occurrences by year\n",
    "merged_data[\"year\"] = pd.to_datetime(merged_data[\"created_utc_comment\"], unit=\"s\").dt.year\n",
    "com_count = merged_data[rel_comments].groupby(\"year\").size().reset_index(name=\"count\")\n",
    "\n",
    "fig = px.line(com_count, x=\"year\", y=\"count\", markers=True, title=\"Mentions of Warren Buffet Over Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ityEIMp-MZpf",
   "metadata": {
    "id": "ityEIMp-MZpf"
   },
   "source": [
    "### Rich-Club Coefficient During Peak Years of Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0HTuQ1fs_u_L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0HTuQ1fs_u_L",
    "outputId": "331701ab-cbbd-4ed6-844d-c8c02767a647"
   },
   "outputs": [],
   "source": [
    "# Function to build a graph\n",
    "def build_graph(df):\n",
    "    G = nx.Graph()\n",
    "    for _, row in df.iterrows():\n",
    "        user = row[\"author_comment\"]\n",
    "        reply_to = row[\"unique_id\"]\n",
    "        G.add_node(user)\n",
    "        if pd.notna(reply_to):\n",
    "            G.add_node(reply_to)\n",
    "            G.add_edge(user, reply_to)\n",
    "    return G\n",
    "\n",
    "# Filter dataset for peak years\n",
    "peak_years = [2019,2020,2021]\n",
    "filtered_data = merged_data[(rel_comments) & (merged_data[\"year\"].isin(peak_years))]\n",
    "\n",
    "# Build the graph\n",
    "G_warren = build_graph(filtered_data)\n",
    "\n",
    "# Compute rich club coefficient\n",
    "rich_club = nx.rich_club_coefficient(G_warren, normalized=False)\n",
    "\n",
    "degrees = list(rich_club.keys())\n",
    "coefficients = list(rich_club.values())\n",
    "\n",
    "# Extend the degree range\n",
    "extended_degrees = np.linspace(min(degrees), max(degrees) + 20, 100)\n",
    "interpolated_coefficients = np.interp(extended_degrees, degrees, coefficients)\n",
    "\n",
    "# Plot the rich club coefficient with extended degree scale\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(extended_degrees, interpolated_coefficients, marker=\"o\", linestyle=\"-\", color=\"b\", label=\"Rich Club Coefficient\")\n",
    "plt.xlim([0, max(degrees) + 10])\n",
    "plt.ylim([0, max(coefficients) + 0.1])\n",
    "plt.xlabel(\"Degree (k)\")\n",
    "plt.ylabel(\"Rich Club Coefficient φ(k)\")\n",
    "plt.title(\"Extended Rich Club Coefficient for Warren Buffet Related Discussions (2019-2021)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8WdR8bcUMzaO",
   "metadata": {
    "id": "8WdR8bcUMzaO"
   },
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gukFHSavMqVy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "gukFHSavMqVy",
    "outputId": "25376b52-0632-4321-bb20-441b1bcc9272"
   },
   "outputs": [],
   "source": [
    "# Function to compute the size of the largest connected component\n",
    "def largest_component_size(G):\n",
    "    if len(G.nodes()) == 0:\n",
    "        return 0\n",
    "    largest_component = max(nx.connected_components(G), key=len)\n",
    "    return len(largest_component) / len(G.nodes()) * 100\n",
    "\n",
    "# Sensitivity analysis function for a specific topic\n",
    "def sensitivity_analysis_for_topic(data, fraction_steps=10):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Build the graph for the filtered topic data\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        author = row['author_comment']\n",
    "        parent = row['unique_id']\n",
    "\n",
    "        if pd.notna(parent) and parent in filtered_data['unique_id'].values:\n",
    "            G.add_edge(author, parent)\n",
    "\n",
    "    print(f\"Graph contains {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
    "\n",
    "    # If graph is empty, return empty DataFrame instead of tuple\n",
    "    if len(G.nodes()) == 0:\n",
    "        return pd.DataFrame(columns=[\"Fraction of Nodes Removed\", \"Largest Component Size (%)\"])\n",
    "\n",
    "    # Prepare for sensitivity analysis\n",
    "    node_count = len(G.nodes())\n",
    "    fractions = np.linspace(0, 1, fraction_steps)  # Fraction of nodes to remove\n",
    "    largest_component_sizes = []\n",
    "\n",
    "    # Sorted list of nodes based on degree (you can adjust this as needed)\n",
    "    sorted_nodes = sorted(G.nodes(), key=lambda x: G.degree(x), reverse=True)\n",
    "\n",
    "    for fraction in fractions:\n",
    "        # Number of nodes to remove\n",
    "        nodes_to_remove = int(fraction * node_count)\n",
    "\n",
    "        # Remove nodes progressively based on the sorted list\n",
    "        nodes_removed = sorted_nodes[:nodes_to_remove]\n",
    "        G.remove_nodes_from(nodes_removed)\n",
    "\n",
    "        # Compute the size of the largest connected component\n",
    "        largest_size = largest_component_size(G)\n",
    "        largest_component_sizes.append(largest_size)\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        \"Fraction of Nodes Removed\": fractions,\n",
    "        \"Largest Component Size (%)\": largest_component_sizes\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Perform sensitivity analysis on Topic 2\n",
    "result_topic = sensitivity_analysis_for_topic(merged_data)\n",
    "\n",
    "if not result_topic.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(result_topic[\"Fraction of Nodes Removed\"], result_topic[\"Largest Component Size (%)\"], label=\"Topic 2\", color=\"red\")\n",
    "    plt.xlabel(\"Fraction of Nodes Removed from the network\")\n",
    "    plt.ylabel(\"Size of Largest Component (%)\")\n",
    "    plt.title(\"Sensitivity Analysis\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for this topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fge8RwT-1fbM",
   "metadata": {
    "id": "fge8RwT-1fbM"
   },
   "outputs": [],
   "source": [
    "# rich_club_df = pd.DataFrame(list(rich_club.items()), columns=['Degree', 'Coefficient'])\n",
    "# rich_club_df.to_csv(\"rich_club_coefficient.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mNaKvmlXTyQF",
   "metadata": {
    "id": "mNaKvmlXTyQF"
   },
   "source": [
    "## Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yE0kJUR3V3CV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "yE0kJUR3V3CV",
    "outputId": "9140da7f-af37-44c0-8089-68a2bf2b6299"
   },
   "outputs": [],
   "source": [
    "# Calculate Z-score for the 'count' of mentions per year\n",
    "mean_count = com_count['count'].mean()  # Mean of counts\n",
    "std_count = com_count['count'].std()  # Standard deviation of counts\n",
    "\n",
    "# Compute the Z-score for each year\n",
    "com_count['z_score'] = (com_count['count'] - mean_count) / std_count\n",
    "\n",
    "# Create an interactive plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for Z-scores vs. count of Warren Buffett mentions\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=com_count['count'],\n",
    "    y=com_count['z_score'],\n",
    "    mode='markers+lines',\n",
    "    marker=dict(size=12, color=com_count['z_score'], colorscale='Viridis', showscale=True),\n",
    "    text=com_count['year'],\n",
    "    hoverinfo='text+name',\n",
    "))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Z-Score vs. Count of Topics Related to Warren Buffett\",\n",
    "    xaxis_title=\"Number of Mentions of Warren Buffett\",\n",
    "    yaxis_title=\"Z-Score\",\n",
    "    hovermode='closest',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    xaxis=dict(showgrid=True),\n",
    "    yaxis=dict(showgrid=True),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UcAoWBEoX9Zp",
   "metadata": {
    "id": "UcAoWBEoX9Zp"
   },
   "source": [
    "##### The Z-scores reveal how public interest in Warren Buffett fluctuates over time. Years with negative Z-scores (e.g., -0.87) show when mentions were below average, likely during quieter periods in his career. In contrast, positive Z-scores, particularly the peak of 1.94, highlight years of exceptional interest, possibly due to significant events like major investments or public statements. The data suggests that Buffett’s media presence and influence are dynamic, with certain years standing out for higher visibility, while others show a dip in public engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MC4gywMlIkMJ",
   "metadata": {
    "id": "MC4gywMlIkMJ"
   },
   "source": [
    "### Monthly Engagement Trends: All Users vs Super Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8685f-becd-46f3-aacc-61d030c684aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "03b8685f-becd-46f3-aacc-61d030c684aa",
    "outputId": "d6a79ba3-4ace-4585-8c82-31fa16bcf577"
   },
   "outputs": [],
   "source": [
    "merged_data[\"month\"] = merged_data[\"created_utc_comment\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# Overall Monthly Engagement of All Users\n",
    "monthly_total_all = merged_data.groupby(\"month\").size().reset_index(name=\"total_comments\")\n",
    "\n",
    "fig_all = px.line(\n",
    "    monthly_total_all, x=\"month\", y=\"total_comments\", markers=True,\n",
    "    title=\"Overall Engagement Trends of All Users\",\n",
    "    labels={\"month\": \"Month\", \"total_comments\": \"Total Comments\"},\n",
    ")\n",
    "\n",
    "fig_all.update_layout(hovermode=\"x\", xaxis_tickangle=-45)\n",
    "fig_all.show()\n",
    "\n",
    "# Monthly Engagement Trends for Super Users\n",
    "super_user_data = merged_data[merged_data[\"author_comment\"].isin(top_5_users)]\n",
    "\n",
    "# Aggregate data by month for super users\n",
    "monthly_total_super = super_user_data.groupby(\"month\").size().reset_index(name=\"total_comments\")\n",
    "\n",
    "fig_super = px.line(\n",
    "    monthly_total_super, x=\"month\", y=\"total_comments\", markers=True,\n",
    "    title=\"Overall Engagement Trends of Super Users\",\n",
    "    labels={\"month\": \"Month\", \"total_comments\": \"Total Comments\"},\n",
    ")\n",
    "\n",
    "fig_super.update_layout(hovermode=\"x\", xaxis_tickangle=-45)\n",
    "fig_super.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgi-_oarQY0_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fgi-_oarQY0_",
    "outputId": "44ef2997-0ea0-4016-bb4f-912f5d531df5"
   },
   "outputs": [],
   "source": [
    "filter_dts=merged_data.copy()\n",
    "\n",
    "# Filter the data to include only entries after 2020\n",
    "filter_dts = filter_dts[filter_dts[\"created_utc_comment\"] >= \"2020-01-01\"]\n",
    "\n",
    "# Create a new column for the week (using \"W-Mon\" to make the week start on Monday)\n",
    "filter_dts[\"week\"] = filter_dts[\"created_utc_comment\"].dt.to_period(\"W-Mon\").astype(str)\n",
    "\n",
    "# Filter only top 5 super users\n",
    "super_user_data = filter_dts[filter_dts[\"author_comment\"].isin(top_5_users)]\n",
    "\n",
    "# Aggregate data by week for super users and all users\n",
    "weekly_activity_all = filter_dts.groupby([\"week\", \"author_comment\"]).size().reset_index(name=\"comment_count\")\n",
    "weekly_activity_super = super_user_data.groupby([\"week\", \"author_comment\"]).size().reset_index(name=\"comment_count\")\n",
    "\n",
    "# Overall Engagement Trends - Total Comments Per Week for All Users\n",
    "weekly_total_all = filter_dts.groupby(\"week\").size().reset_index(name=\"total_comments\")\n",
    "\n",
    "fig2_all = px.line(\n",
    "    weekly_total_all, x=\"week\", y=\"total_comments\", markers=True,\n",
    "    title=\"Overall Engagement Trends of All Users (Weekly After 2020)\",\n",
    "    labels={\"week\": \"Week\", \"total_comments\": \"Total Comments\"},\n",
    ")\n",
    "\n",
    "fig2_all.update_layout(hovermode=\"x\", xaxis_tickangle=-45)\n",
    "fig2_all.show()\n",
    "\n",
    "# Overall Engagement Trends - Total Comments Per Week for Super Users\n",
    "weekly_total_super = super_user_data.groupby(\"week\").size().reset_index(name=\"total_comments\")\n",
    "\n",
    "fig2_super = px.line(\n",
    "    weekly_total_super, x=\"week\", y=\"total_comments\", markers=True,\n",
    "    title=\"Overall Engagement Trends of Super Users (Weekly After 2020)\",\n",
    "    labels={\"week\": \"Week\", \"total_comments\": \"Total Comments\"},\n",
    ")\n",
    "\n",
    "fig2_super.update_layout(hovermode=\"x\", xaxis_tickangle=-45)\n",
    "fig2_super.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qq12ybQ_GN8X",
   "metadata": {
    "id": "qq12ybQ_GN8X"
   },
   "source": [
    "### Analyzing Comments During Specific Time Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sL2JVdxrGiXg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sL2JVdxrGiXg",
    "outputId": "c4be1f1a-8c2d-45c4-a19c-ecbd52c7769e"
   },
   "outputs": [],
   "source": [
    "# Define the two time periods\n",
    "period_1_start = \"2021-01-26\"\n",
    "period_1_end = \"2021-02-01\"\n",
    "period_2_start = \"2022-01-04\"\n",
    "period_2_end = \"2022-01-10\"\n",
    "\n",
    "# Filter for comments within the specified time periods\n",
    "df_period_1 = merged_data[\n",
    "    (merged_data[\"created_utc_comment\"] >= period_1_start) &\n",
    "    (merged_data[\"created_utc_comment\"] <= period_1_end)\n",
    "]\n",
    "\n",
    "df_period_2 = merged_data[\n",
    "    (merged_data[\"created_utc_comment\"] >= period_2_start) &\n",
    "    (merged_data[\"created_utc_comment\"] <= period_2_end)\n",
    "]\n",
    "\n",
    "# Display the number of comments in each period\n",
    "print(f\"Number of comments in {period_1_start} - {period_1_end}: {df_period_1.shape[0]}\")\n",
    "print(f\"Number of comments in {period_2_start} - {period_2_end}: {df_period_2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i3U3v7nuHBfQ",
   "metadata": {
    "id": "i3U3v7nuHBfQ"
   },
   "outputs": [],
   "source": [
    "# df_period_1.to_csv(\"df_period_1.csv\")\n",
    "# df_period_2.to_csv(\"df_period_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-h7CxKkUGDSc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "-h7CxKkUGDSc",
    "outputId": "46ab0826-23ff-4abd-b30f-8905d1160a97"
   },
   "outputs": [],
   "source": [
    "df_period_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7CFnuP6HGzN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "c7CFnuP6HGzN",
    "outputId": "574d1673-4919-4ae7-8895-ab7e8f5aae3b"
   },
   "outputs": [],
   "source": [
    "df_period_2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
